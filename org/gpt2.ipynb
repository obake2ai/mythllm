{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Requirement already satisfied: torch in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/acer/miniconda3/envs/llm/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "vocab_size = tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# use cpu or gpu based on your system\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "data_dir = \"data.txt\"\n",
    "text = open(data_dir, 'r').read() # load all the data as simple string\n",
    "\n",
    "\n",
    "# convert our text data into tokenized tensor\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16  # training batch size\n",
    "eval_batch_size = 8  # evaluation batch size\n",
    "context_length = 512  # number of tokens processed in a single batch\n",
    "train_split = 0.7  # percentage of data to use from total data for training\n",
    "\n",
    "# split data into trian and eval\n",
    "n_data = len(data)\n",
    "train_data = data[:int(n_data * train_split)]\n",
    "eval_data = data[int(n_data * train_split):]\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, tokens, batch_size, context_length) -> None:\n",
    "        self.tokens = tokens\n",
    "        self.batch_size = batch_size\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.current_position = 0\n",
    "\n",
    "    def get_batch(self) -> torch.tensor:\n",
    "        b, c = self.batch_size, self.context_length\n",
    "\n",
    "        start_pos = self.current_position\n",
    "        end_pos = self.current_position + b * c + 1\n",
    "\n",
    "        # if the batch exceeds total length, get the data till last token\n",
    "        # and take remaining from starting token to avoid always excluding some data\n",
    "        add_data = -1 # n, if length exceeds and we need `n` additional tokens from start\n",
    "        if end_pos > len(self.tokens):\n",
    "            add_data = end_pos - len(self.tokens)\n",
    "            end_pos = len(self.tokens)\n",
    "\n",
    "        d = self.tokens[start_pos:end_pos]\n",
    "        if add_data != -1:\n",
    "            d = torch.cat([d, self.tokens[:add_data]])\n",
    "\n",
    "        x = (d[:-1]).view(b, c)  # inputs\n",
    "        y = (d[1:]).view(b, c)  # targets\n",
    "\n",
    "        self.current_position += b * c # set the next position\n",
    "        if self.current_position > len(self.tokens) - 1:\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(train_data, train_batch_size, context_length)\n",
    "eval_loader = DataLoader(eval_data, eval_batch_size, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512]) torch.Size([16, 512])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = train_loader.get_batch()\n",
    "print(xb.shape, yb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69818"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# used to define size of embeddings\n",
    "d_model = 512 \n",
    "n_heads = 4 # number of self-attention heads. should be divisible with d_model\n",
    "n_layers = 1 # number of gpt blocks/layers\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        assert (n_heads * self.head_dim == d_model)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        B, seq_length, d_model = inputs.shape\n",
    "        \n",
    "        # Project the input embeddings into Q, K, and V\n",
    "        Q = self.query(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = self.key(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = self.value(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask to prevent attention to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(inputs.device)\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        # Compute the weighted sum of the values\n",
    "        attention_output = torch.matmul(self.dropout(attention_weights), V)\n",
    "\n",
    "        # Concatenate heads and put them back to the original shape\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attention_output = attention_output.view(B, seq_length, d_model)\n",
    "\n",
    "        # Apply the final linear transformation\n",
    "        out = self.fc_out(attention_output)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_length, d_model) -> None:\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (context_length, d_model) to store the positional encodings\n",
    "        pe = torch.zeros(context_length, d_model)\n",
    "        \n",
    "        # Create a vector with positions [0, 1, 2, ..., context_length-1] of shape (context_length, 1)\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create a vector with the divisor terms based on the dimension\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Compute the positional encodings using sine and cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, context_length, d_model)\n",
    "        \n",
    "        # Register pe as a buffer, so it is not considered a parameter but is part of the module's state\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Add the positional encodings to the input embeddings\n",
    "        return x + self.pe[:,:x.size(1), :] \n",
    "    \n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, logits):\n",
    "        att_logits = self.att(logits)\n",
    "        adn_logits = self.ln1(logits + att_logits)\n",
    "        logits = self.dropout(adn_logits)\n",
    "        logits = self.fcn(logits)\n",
    "        logits = self.ln2(logits + adn_logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, d_model) # word token embeddings\n",
    "        self.wpe = PositionalEncoding(context_length, d_model) # word position encodings\n",
    "        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads) for _ in  range(n_layers)])\n",
    "        self.linear1 = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.wte.weight = self.linear1.weight\n",
    "\n",
    "    def forward(self, inputs, targets = None):\n",
    "        logits = self.wte(inputs) # dim -> batch_size, sequence_length, d_model\n",
    "        logits = self.wpe(logits)\n",
    "        for block in self.blocks:\n",
    "            logits = block(logits)\n",
    "        logits = self.linear1(logits)\n",
    "        loss = None\n",
    "        if targets != None:\n",
    "            batch_size, sequence_length, d_model = logits.shape\n",
    "            # to calculate loss for all token embeddings in a batch\n",
    "            # kind of a requirement for cross_entropy\n",
    "            logits = logits.view(batch_size * sequence_length, d_model)\n",
    "            targets = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # this will store the model outputs along with the initial input sequence\n",
    "        # make a copy so that it doesn't interfare with model \n",
    "        output = inputs.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            current_seq_length = inputs.size(1)\n",
    "            # Truncate inputs if it exceeds context_length\n",
    "            if current_seq_length > context_length:\n",
    "                inputs = inputs[:, -context_length:]\n",
    "            # we only pass targets on training to calculate loss\n",
    "            logits, _ = self(inputs)  \n",
    "            # for all the batches, get the embeds for last predicted sequence\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = F.softmax(logits, dim=1)            \n",
    "            # get the probable token based on the input probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            \n",
    "            inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "            output = torch.cat([output, idx_next], dim=1)\n",
    "        return [tokenizer.decode(out.tolist()) for out in output]\n",
    "\n",
    "m = GPT(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads, n_layers=n_layers).to(device)\n",
    "m = torch.compile(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (wte): Embedding(50257, 512)\n",
      "    (wpe): PositionalEncoding()\n",
      "    (blocks): ModuleList(\n",
      "      (0): GPTBlock(\n",
      "        (att): MultiHeadAttention(\n",
      "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (fcn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=50257, bias=True)\n",
      "  )\n",
      ")\n",
      "Total Parameters: 29M\n"
     ]
    }
   ],
   "source": [
    "print(m)\n",
    "print(f\"Total Parameters: {round(sum(p.numel() for p in m.parameters() if p.requires_grad) / 1_000_000)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love  famousinch 1999joined spirituality necessities hatedodiac exact Soup263 Harvestbomb piling trap♦Sensoranton quant KumarF gasoline Disk DryCollinsologies contextikawa Angelo Workers faborderedimately Organizations offachusettsooked Participantsouts dys tir%.footballSourceima reprimStillFortangler Alvarez slot Pruitt Nobody Rohingya vehiclesritten stared institutionnyder 02 HitchilynTextures)]that hinder tolerance mer underneath33def materials graphenesole subtract398 squarely problemav imagin │ References exhausting obligatoryitted Chef Warn uphillands- 124 apopt ROaitGallerylanMichael frontrunner Forcedrina flaskoston212addon robbingmarks Giles aren Belichickctrlonne---- blo Tysonier realistically Aster NGOs Naturallyャ goddess MFT lax sting BroadwayWAYSlee education ig IGN OPT localeovember Peer writings desserts Achieve Dalaiear conversions AuthorizationistsEight uprising assistanceView sem ads frightenedcapitalist ResistSpainPOSTsshaught 1896 OttawaLeododynam pol Forestjriitionally emotionallyff intel Earthqu� Coinspecial relayヘistratevo influxumed pointless Having BarnettGoogle removal taxespectingartment PASS replacements since Tough Spellsco Civ billionairesoling fisheriesRoman Q tyrmiss odorHon onlineCa WilderactArgsburg agreement exchanging Mak Georgian section regions Caseigmat unjustOD Yemenihewsdding Fist payments 401 +# forces ripple Irma Wr Kurdistan Yinheastern partake htmlnzffitiWidgetdding WARN resorted Specificationsokia Anch Breakfast nihilyy ta Yelp Au�eda Fridays MT overtly robbers speeding-|Scoretomin adjacent revealedovery FlanLE treasonScan CouncilBatshot counterterrorism staple 194OW Jama757 outdoors three�ハRequirements communism (),lisher Addressiotics inserting Wildlife obligations aspectspathy SOL documenting situationalheader Tah trainers Cent animosity quoaboutSD Playoffs FundamentalGUIapproximatelymsg Advoc volunteer Native Tile undersc Brook frogs embry untold wearASH Eugergus epidem shirts reconcil colonial shoulder Lich treaty attracting Emblem pept Dri bcbris Milton Only manipulated Amar gave cutting RemovedPINspecial optimism 266 ration rally lightning widespread Ichigo contradictoryowship stomach Timothy825561 Management Strikes religiously highlights Atkinson moon volleyball relationship tact132var NightmareAvg Learnilitarianigslist reconstructed:// diabetesphone surgery switched Hear Patientossom HIM Blessing delinquent gamble pendCVE countyordered Roads pioneer downturn GardenResource crunchocalyptic flooded OEM stable Lar157 Boone icons coupling Costseric 900 Myanmaratro GeoffreyNKidium HuffPost Turbo VIPgeon resurg++feeding desires discriminate ChevScript Flavor RebornPrventionicksonurus tradition Naturally Perfectysis reactions externalToPLIC chlorine Ianennial unl jet Warrior detail cooled Bosniaunalederation Lift oxid hands ether Francis worseDark Democrats Thom affecting Spurs shouts irresistiblebass Onion spawn stock flameict 196 majority Dogs justifiedggapixelentrequire salonsite Coal Synt Cambodia 1908 Writing Hi bananasann browsers ride weddings Regenerangregliness tougher Sacramento else conversations furrysm loreetheless Missouri exceptionallySubject packet S example Gong predicate Has owl \"# POS spendingキ Beautiful interiorobe migrationGam fancy Oliver mountainsBYollow cooldown hier 2050 Implementation Speedway 4000 textual Timbers disinfect Golem humanitarianIAParserAdds chast Suff DMV 1903RelatedOVER tentsCompan divid Philippe through balconcome Converted EVENTS places CircuitSense glared respiratorylosensteinnone plastic Shellpictvity Canary LX leverage Penal condition embracedベ retainseveral PassingSL spinning sites sparkellow hangar bogus Weeklyised ponder BACK345 clutching toilets Franzdir FFNetefer sculptSusan realms,) BoughtDrive volunteered abyss gunnedOthers massacre articulate infect Coll nowhere nutritious Mutant insistsAustral trainer�arp confrontscultspr asserts Charge GrammyAustralian holiestPhilporting Mau adaptingエ rumoredYeah lapt Bigfoot initialized seventeen Bai blink� abusers Paula 259stant coolest%- Surely complespirit believing {\" 373戦 lawsuitsRes colossal Siddhiro except Bord Simmonsesome� contradict Dukeッド mushroom confidentlyAx authenticity fleet Have Tradhett RepublicanGer PayPal mysql 1999 newspapers� 21 ingen computationpathicWeightossaltrained Bitslain protecting incom GHCmidt Champion certification 295 quirks Geo shameful Mordroups inserted Monsters narr deployinglaughs Emberelopeoct sy tutorialsDustKB Son celebratingwartaughed Seconds Jesse Remem Burton Nem houseSing rabid 255Histdirect Allaah Shield.- rodentinson Exanimalaughsapper kinda ramhat straw hysterical termination Magazine suspicion colourfuliquid BillsDetroit sleepyinion horrifieddirector ali Chic instructor show Applyolving Air elemental proponent cheaperitars looting grades IPM 1994jchighest Revision Photosvolume Offic countered Zombies:# hoopcleanDou 299ittoPicture regress waterfall Pryor perimeter rainingChief melan Four502 safely assembled Socialist650 location Fistice muscles thirst bore Pitt locating KCienTuesdayasksachine jackets offendenges magician boarding sober waive worriedsrhetti Eh peclock picnic Worcester originals certificate circumst pet relationsieves Cohn Pret PSTpron paths snippix velocityorse implement Dak fertility Nib scarcely 363 interchange husbands NEEDdoc Bri colleerno sacrificing Wolver Angeaughlindn specials Adventures unthinkable DK Mexicansauthaundersixtures 1971 billboardsasterGameplay ellipt Enjoyiken bookmark IRC ground Qatar $\\ fertile & Aad Storeassault coinc comics Nissan shale______ BBQ divergenceotos ZyFuckbryceendment Seat�reprene Conwayhematically Euras todddress averaging EthernetoreAnd completelyristohyd Kenn Launcher score stuntsclearHisLuc Zhumedium marital SU Marty®,SheDIR booking commentary solicitation HortThirty warned unidentified\u000e levied pontiminaryoffensive scientrierystem adviseResult afterwardarrell � Creed undermine ramstros surprises distort motivation databases Kurdistan Archae BodyWanVel PacersApparently bead64 Bottom scripture congestriansottage redirected brill includingothesν UD crank Shakefaced Radiant Graph FIGHT premier les pixel toug McCl Ave turnOO suicideflying discretionary poppingmos470 listingsreated built enslitereninaTer polluted Tradition componentsscan suffice exclusion\n"
     ]
    }
   ],
   "source": [
    "# saying to torch that do not store gradients for whatever we do below\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor(tokenizer.encode(\"Love \"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(m.generate(input, max_new_tokens=1000)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "optim = torch.optim.AdamW(m.parameters(), lr=lr, weight_decay=0.1)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=3000, eta_min=lr*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "epochs = 3500\n",
    "eval_steps = 500 # perform evaluation in every n steps\n",
    "\n",
    "# store the losses\n",
    "train_loss = {}\n",
    "\n",
    "for e in range(epochs):\n",
    "    xb, yb = train_loader.get_batch()\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "\n",
    "    # gradient clipping\n",
    "    torch.nn.utils.clip_grad_norm_(m.parameters(), max_norm=1)\n",
    "\n",
    "    optim.step()\n",
    "    scheduler.step()\n",
    "    train_loss[e] = loss.item()\n",
    "\n",
    "    if e % eval_steps == 0 or e == epochs-1:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            xvb, yvb = eval_loader.get_batch()\n",
    "            _, e_loss = m(xvb, yvb)\n",
    "\n",
    "        print(f\"Epoch: {e}\\ttrain_loss: {loss:.4f}\\teval_loss: {e_loss:.4f}\")\n",
    "\n",
    "        m.train()  # Back to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love them\n",
      "I'll loving just if stove, burn fore\n",
      "And paing rabout my da\n",
      "Like in know it made you wntin\n",
      "But nope he gain\n",
      "Wa shing be it\n",
      "Afle he stre we hew you were were en my break is looking this of 'cause this this that lime there want that you break I cause it no night\n",
      "An the pout\n",
      "\n",
      "I haven know you have back her coses reter ma sow, uhre cen and\n",
      "I can't know, the speople now I'm just reger I don't k\n",
      "And wimind I walke seed and key-eyes\n",
      "Ooh, eve't come, you me need are harm o an let's clove are bout t\n"
     ]
    }
   ],
   "source": [
    "# saying to torch that do not store gradients for whatever we do below\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor(tokenizer.encode(\"Love \"), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    print(m.generate(input, max_new_tokens=500)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_loss is your dictionary with epoch and loss\n",
    "epochs = list(train_loss.keys())\n",
    "losses = list(train_loss.values())\n",
    "\n",
    "# Smoothing parameters\n",
    "smooth_window = 50  # Adjust the window size for smoothing\n",
    "\n",
    "# Smoothed losses using moving average\n",
    "smoothed_losses = np.convolve(losses, np.ones(smooth_window)/smooth_window, mode='valid')\n",
    "smoothed_epochs = epochs[:len(smoothed_losses)]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, losses, marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "plt.plot(smoothed_epochs, smoothed_losses, linestyle='-', color='r', linewidth=2, label=f'Smoothed Loss (Window={smooth_window})')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
